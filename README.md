# Home Sales Analysis

This project demonstrates how to analyze a home sales dataset using **Apache Spark SQL**. The goal is to read the data from an AWS S3 bucket, perform SQL queries on it, and optimize query performance using techniques such as caching and partitioning.

## Project Requirements

- **Apache Spark**
- **Python 3**
- **pyspark** and **findspark** libraries

## Environment

This project was implemented on **Google Colab**, which provides an easy environment to run Spark. Below are the steps to set up and run the project on **Google Colab** or any local environment with Spark installed.

## Conclusion

This project demonstrates how to perform scalable data analysis using **Apache Spark** SQL. By leveraging Spark's powerful distributed computing capabilities, queries that would otherwise be slow or infeasible on large datasets can be executed efficiently. The project also highlights performance optimization techniques like caching and partitioning, which can drastically improve runtime.